behaviors:
  Agent:
    trainer_type: ppo
    hyperparameters:
      batch_size: 1024
      buffer_size: 10240
      learning_rate: 3.0e-4
      beta: 5.0e-4
      epsilon: 0.2
      lambd: 0.99
      num_epoch: 6
      learning_rate_schedule: linear
    network_settings:
      normalize: false
      hidden_units: 256
      num_layers: 3
#------------------------------
    behavioral_cloning:
      demo_path: Assets/Demonstrations/Shoot Laser_1.demo
      strength: 0.7
      steps: 500000
      batch_size: 512
      num_epoch: 3
      samples_per_update: 0
#------------------------------
    reward_signals:
      extrinsic:
        gamma: 0.99
        strength: 1.0
#------------------------------
      curiosity:
        strength: 0.02
        gamma: 0.99
        encoding_size: 256
#------------------------------
      gail:
        strength: 0.1
        gamma: 0.9
        demo_path: Assets/Demonstrations/Shoot Laser_1.demo
        encoding_size: 64
        use_actions: true
#------------------------------
    self_play:
      window: 10
      play_against_latest_model_ratio: 0.5
      save_steps: 50000
      swap_steps: 5000
      team_change: 100000
#------------------------------
    time_horizon: 64
    max_steps: 10000000
    summary_freq: 50000